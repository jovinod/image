{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmo4CGzp+CuxFnL+SSk1Bv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovinod/image/blob/main/Crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UpN1ihqrbOyi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piJF1_fndiD4",
        "outputId": "358cde56-d3b3-4fb0-becf-d8a15f2db7ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# While training a CNN model we may need to transform the input image data by normalizing it. Normalizing is the \n",
        "# process of creating a standard distribution (x - mean)/std. Typically on web you will see some standard values \n",
        "# being used for normalizing e.g. torch.transforms.Normalize((0.5,0.5,05.),(0.5,0.5,0.5)). Here the first vector \n",
        "# is the mean for R, G, B channels and second vector is the standard deviation for R, G, B channel. If the input \n",
        "# data is in the range 0-1, the above transformation will change it between the range -1 to 1 e.g. (0 - 0.5)/0.5\n",
        "# and (1 - 0.5)/0.5\n",
        "\n",
        "# Do we need to normalize our data?\n",
        "img_loader = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder('/content/drive/My Drive/crop/train',\n",
        "                                                                         transform=transforms.ToTensor()), \\\n",
        "                                         batch_size=64, \\\n",
        "                                         shuffle=False, \\\n",
        "                                         num_workers=4)\n",
        "\n",
        "first_batch = iter(img_loader).next()\n",
        "feature, label = first_batch\n",
        "print(f'Min pixel value {feature[:1,:,:].min()}')\n",
        "print(f'Max pixel value {feature[:1,:,:].max()}')\n",
        "\n",
        "\n",
        "# From the output you can see that our data is between 0 and 1. The reason the data is between 0 & 1 is because\n",
        "# we have used a tansform torch.transform.ToTensor which converts the input data in range 0-255 to 0-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieD67_3odRvN",
        "outputId": "da055073-77ec-4165-8a46-16fb5dcc9f36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min pixel value 0.0\n",
            "Max pixel value 0.886274516582489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To normalize our data we need to find the mean and standard deviation for each of the channel. We will use \n",
        "# the existing data to identify these values. Although we would like to find the mean and standard deviation in\n",
        "# a single go using all the records, however we have 70k+ records which may be too large to fit into a single batch\n",
        "# As such we are using the batch size of 4096. At the end we will find a mean of mean and mean of standard deviation\n",
        "\n",
        "img_loader = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder('/content/drive/My Drive/crop/train',\n",
        "                                                                         transform=transforms.ToTensor()), \\\n",
        "                                         batch_size=4096, \\\n",
        "                                         shuffle=False, \\\n",
        "                                         num_workers=4)\n",
        "\n",
        "pop_mean = []\n",
        "pop_std0 = []\n",
        "pop_std1 = []\n",
        "for i, data in enumerate(img_loader, 0):\n",
        "    numpy_image = data[0].numpy()\n",
        "    \n",
        "    # The axis here represents that we are first going to find the mean across the rows (2), then the columns (3)\n",
        "    # and finally across all the images. Eventually we will get one value for each channel\n",
        "    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
        "    batch_std0 = np.std(numpy_image, axis=(0,2,3))\n",
        "    batch_std1 = np.std(numpy_image, axis=(0,2,3), ddof=1) # This is for the degree of freedom N-1\n",
        "    \n",
        "    pop_mean.append(batch_mean)\n",
        "    pop_std0.append(batch_std0)\n",
        "    pop_std1.append(batch_std1)\n",
        "\n",
        "pop_mean = np.array(pop_mean).mean(axis=0)\n",
        "pop_std0 = np.array(pop_std0).mean(axis=0)\n",
        "pop_std1 = np.array(pop_std1).mean(axis=0)\n",
        "\n",
        "print(pop_mean)\n",
        "print(pop_std0)\n",
        "print(pop_std1)"
      ],
      "metadata": {
        "id": "ss4W1NNPdtPP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we create our final transformation that would be used before we send the data for training the model\n",
        "transform = torchvision.transforms.Compose([transforms.ToTensor(), \\\n",
        "                                            transforms.Normalize((0.4743617, 0.49847862, 0.4265874 ), \\\n",
        "                                                                 (0.21134755, 0.19044809, 0.22679578))\n",
        "                                           ]\n",
        "                                          )\n",
        "crop_dataset = torchvision.datasets.ImageFolder('/content/drive/My Drive/crop/train', transform=transform)\n",
        "crop_dataset.class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pizprRbxd9Gb",
        "outputId": "c3795b08-b23a-46cf-852c-96152dee0bdb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Apple___Apple_scab': 0,\n",
              " 'Apple___Black_rot': 1,\n",
              " 'Apple___Cedar_apple_rust': 2,\n",
              " 'Apple___healthy': 3,\n",
              " 'Blueberry___healthy': 4,\n",
              " 'Cherry_(including_sour)___Powdery_mildew': 5,\n",
              " 'Cherry_(including_sour)___healthy': 6,\n",
              " 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7,\n",
              " 'Corn_(maize)___Common_rust_': 8,\n",
              " 'Corn_(maize)___Northern_Leaf_Blight': 9,\n",
              " 'Corn_(maize)___healthy': 10,\n",
              " 'Grape___Black_rot': 11,\n",
              " 'Grape___Esca_(Black_Measles)': 12,\n",
              " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13,\n",
              " 'Grape___healthy': 14,\n",
              " 'Orange___Haunglongbing_(Citrus_greening)': 15,\n",
              " 'Peach___Bacterial_spot': 16,\n",
              " 'Peach___healthy': 17,\n",
              " 'Pepper,_bell___Bacterial_spot': 18,\n",
              " 'Pepper,_bell___healthy': 19,\n",
              " 'Potato___Early_blight': 20,\n",
              " 'Potato___Late_blight': 21,\n",
              " 'Potato___healthy': 22,\n",
              " 'Raspberry___healthy': 23,\n",
              " 'Soybean___healthy': 24,\n",
              " 'Squash___Powdery_mildew': 25,\n",
              " 'Strawberry___Leaf_scorch': 26,\n",
              " 'Strawberry___healthy': 27,\n",
              " 'Tomato___Bacterial_spot': 28,\n",
              " 'Tomato___Early_blight': 29,\n",
              " 'Tomato___Late_blight': 30,\n",
              " 'Tomato___Leaf_Mold': 31,\n",
              " 'Tomato___Septoria_leaf_spot': 32,\n",
              " 'Tomato___Spider_mites Two-spotted_spider_mite': 33,\n",
              " 'Tomato___Target_Spot': 34,\n",
              " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 35,\n",
              " 'Tomato___Tomato_mosaic_virus': 36,\n",
              " 'Tomato___healthy': 37}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can create a loader that will help us load images in batches for training purpose \n",
        "crop_loader = DataLoader(crop_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "CNboqUTweYIX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature, label = iter(crop_loader).next()\n",
        "fig, axes = plt.subplots(figsize=(200,100), nrows=16, ncols=8)\n",
        "for i in range(16):\n",
        "    for j in range(8):\n",
        "        ax = axes[i][j]\n",
        "        ax.imshow((feature[(8*i)+j]).permute(1, 2, 0))\n",
        "        ax.title.set_text(' '.join('%5s' % os.path.basename(crop_dataset.imgs[(8*i)+j][0])))"
      ],
      "metadata": {
        "id": "4P_f9yk1ea1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we look at a single image. Our intent is to see how does an image transform through the convolution model that \n",
        "# we are proposing\n",
        "img_one_feature = feature[:1]\n",
        "img_one_label = label[:1]\n",
        "img_one_feature.shape, img_one_label.shape\n",
        "\n",
        "plt.imshow(img_one_feature[0].permute(1,2,0))\n",
        "\n",
        "# We define a convolution that converts the RGB channel into 6 features/filters/channels using a kernel size of 3 \n",
        "# a stride of 1 and padding of 1. On this we would apply pooling of 2*2\n",
        "cnv1 = nn.Conv2d(3, 6, kernel_size=9, padding=1, stride=1)\n",
        "#print(cnv1.weight, cnv1.bias)\n",
        "layer1 = cnv1(img_one_feature)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=6)\n",
        "for i in range(6):\n",
        "    x = torch.tensor(layer1[0,i:i+1,:], requires_grad=False)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(x.permute(1, 2, 0))\n",
        "\n",
        "layer1 = F.relu(layer1)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=6)\n",
        "for i in range(6):\n",
        "    x = torch.tensor(layer1[0,i:i+1,:], requires_grad=False)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(x.permute(1, 2, 0))\n",
        "\n",
        "pool = nn.MaxPool2d(2, 2)    \n",
        "layer1 = pool(layer1)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=6)\n",
        "for i in range(6):\n",
        "    x = torch.tensor(layer1[0,i:i+1,:], requires_grad=False)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(x.permute(1, 2, 0))\n",
        "print()\n",
        "\n",
        "\n",
        "# We define a convolution that converts the RGB channel into 12 features/filters/channels using a kernel size of 3 \n",
        "# a stride of 1 and padding of 1. On this we would apply pooling of 2*2\n",
        "cnv2 = nn.Conv2d(6, 12, kernel_size=6, padding=1, stride=1)\n",
        "#print(cnv2.weight, cnv1.bias)\n",
        "layer2 = cnv2(layer1)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=12)\n",
        "for i in range(12):\n",
        "    x = torch.tensor(layer2[0,i:i+1,:], requires_grad=False)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(x.permute(1, 2, 0))\n",
        "\n",
        "layer2 = F.relu(layer2)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=12)\n",
        "for i in range(12):\n",
        "    x = torch.tensor(layer2[0,i:i+1,:], requires_grad=False)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(x.permute(1, 2, 0))\n",
        "\n",
        "pool = nn.MaxPool2d(2, 2)    \n",
        "layer2 = pool(layer2)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=12)\n",
        "for i in range(12):\n",
        "    x = torch.tensor(layer2[0,i:i+1,:], requires_grad=False)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(x.permute(1, 2, 0))\n",
        "print()\n",
        "\n",
        "\n",
        "# We define a convolution that converts the RGB channel into 12 features/filters/channels using a kernel size of 3 \n",
        "# a stride of 1 and padding of 1. On this we would apply pooling of 2*2\n",
        "cnv3 = nn.Conv2d(12, 36, kernel_size=3, padding=1, stride=1)\n",
        "#print(cnv3.weight, cnv1.bias)\n",
        "layer3 = cnv3(layer2)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=12, nrows=3)\n",
        "for i in range(3):\n",
        "    for j in range(12):\n",
        "        x = torch.tensor(layer3[0,(12*i) + j: (12*i) + j + 1,:], requires_grad=False)\n",
        "        ax = axes[i][j]\n",
        "        ax.imshow(x.permute(1, 2, 0))\n",
        "\n",
        "layer3 = F.relu(layer3)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=12, nrows=3)\n",
        "for i in range(3):\n",
        "    for j in range(12):\n",
        "        x = torch.tensor(layer3[0,(12*i) + j: (12*i) + j + 1,:], requires_grad=False)\n",
        "        ax = axes[i][j]\n",
        "        ax.imshow(x.permute(1, 2, 0))\n",
        "\n",
        "pool = nn.MaxPool2d(2, 2)    \n",
        "layer3 = pool(layer3)\n",
        "fig, axes = plt.subplots(figsize=(200,100), ncols=12, nrows=3)\n",
        "for i in range(3):\n",
        "    for j in range(12):\n",
        "        x = torch.tensor(layer3[0,(12*i) + j: (12*i) + j + 1,:], requires_grad=False)\n",
        "        ax = axes[i][j]\n",
        "        ax.imshow(x.permute(1, 2, 0))\n",
        "print()"
      ],
      "metadata": {
        "id": "hn72zy-UedoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now create a a model that can be trained for disease detection\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 9)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 12, 6)\n",
        "        self.conv3 = nn.Conv2d(12, 18, 3)\n",
        "        self.fc1 = nn.Linear(18 * 28 * 28, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 512)\n",
        "        self.fc4 = nn.Linear(512, 38)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pEKYKSkfejde"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "fB78fYWHewcz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in net.state_dict():\n",
        "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jakhqjx5e5bd",
        "outputId": "3973360f-7f56-4464-e104-403cee148f6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "conv1.weight \t torch.Size([6, 3, 9, 9])\n",
            "conv1.bias \t torch.Size([6])\n",
            "conv2.weight \t torch.Size([12, 6, 6, 6])\n",
            "conv2.bias \t torch.Size([12])\n",
            "conv3.weight \t torch.Size([18, 12, 3, 3])\n",
            "conv3.bias \t torch.Size([18])\n",
            "fc1.weight \t torch.Size([4096, 14112])\n",
            "fc1.bias \t torch.Size([4096])\n",
            "fc2.weight \t torch.Size([1024, 4096])\n",
            "fc2.bias \t torch.Size([1024])\n",
            "fc3.weight \t torch.Size([512, 1024])\n",
            "fc3.bias \t torch.Size([512])\n",
            "fc4.weight \t torch.Size([38, 512])\n",
            "fc4.bias \t torch.Size([38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ssYuQLe7oy",
        "outputId": "7e0bcb77-f23c-4a9c-bdc2-b4101c546b9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dPS-KdNe939",
        "outputId": "fa2c10ed-44ee-4636-8525-f4cc69ca4805"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCvSLMlkfAC6",
        "outputId": "6663a309-ce5b-4f67-b48b-15a92d99b7f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(9, 9), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 12, kernel_size=(6, 6), stride=(1, 1))\n",
              "  (conv3): Conv2d(12, 18, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=14112, out_features=4096, bias=True)\n",
              "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc4): Linear(in_features=512, out_features=38, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.train()\n",
        "    for i, data in enumerate(crop_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        if i % 500 == 499:    # print every 2000 mini-batches\n",
        "            accu=100.*correct/total\n",
        "            \n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 500:.3f} accuracy:{accu:.3f}')\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "uNftDGJUfB8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), '/content/drive/My Drive/crop/model')"
      ],
      "metadata": {
        "id": "z9M2rkg1fPBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}